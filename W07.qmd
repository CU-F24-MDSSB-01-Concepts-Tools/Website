---
title: "W#07: Linear Model, Fitting, Interaction Effects, Nonlinear Models"
author: Jan Lorenz
format: 
  revealjs: 
    toc: true
    toc-depth: 1
    slide-number: true
    chalkboard: 
      buttons: true
    preview-links: true
    logo: img/ConstructorUniversity.png
    footer: "MDSSB-DSCO-02: Data Science Concepts"
bibliography: "/home/janlo/Documents/literature/litlorenz_zot.bib"
editor_options: 
  chunk_output_type: console
---

```{r}
# Check if we replace Interaction Effects with Feature Engineering following Chelsea
library(tidyverse)
```



# Linear Model
The first work-horse to explore relations between numerical variables

## Different purposes of models {.smaller background-color="khaki"}

**Agent-based models** and **differential equations** are usually used to **explain** the **dynamics** of one or more variables typically over time. They are used to answer [*mechanistic questions*]{style='color:blue;'}.

. . . 

In the following we treat **variable-based models** which we use to 

- **explain** relations between variables
- make **predictions**

These are often used to answer [*inferential*]{style='color:blue;'} and [*predictive questions*]{style='color:blue;'}.   
(With experimental or more theoretical effort also for [*causal questions*]{style='color:blue;'}.)

First, we focus on linear models.

## Hello again penguins! {.scrollable .smaller}

We use the dataset [Palmer Penguins](https://allisonhorst.github.io/palmerpenguins/)

Chinstrap, Gentoo, and AdÃ©lie Penguins

![](https://upload.wikimedia.org/wikipedia/commons/0/08/South_Shetland-2016-Deception_Island%E2%80%93Chinstrap_penguin_%28Pygoscelis_antarctica%29_04.jpg){height=200}
![](https://upload.wikimedia.org/wikipedia/commons/0/00/Brown_Bluff-2016-Tabarin_Peninsula%E2%80%93Gentoo_penguin_%28Pygoscelis_papua%29_03.jpg){height=200}
![](https://upload.wikimedia.org/wikipedia/commons/e/e3/Hope_Bay-2016-Trinity_Peninsula%E2%80%93Ad%C3%A9lie_penguin_%28Pygoscelis_adeliae%29_04.jpg){height=200}


```{r}
library(palmerpenguins)
penguins
```

## Body mass in grams

```{r}
#| echo: true
penguins |>
  ggplot(aes(body_mass_g)) +
  geom_histogram()
```

## Flipper length in millimeters

```{r}
#| echo: true
penguins |>
  ggplot(aes(flipper_length_mm)) +
  geom_histogram()
```

## Relate variables as a line {.smaller background-color="aquamarine"}

A *line* is a shift-scale transformation of the identity function usually written in the form 

$$f(x) = a\cdot x + b$$

where [$a$ is the *slope*]{style="color:red;"}, [$b$ is the *intercept*]{style="color:blue;"}.^[This a scale and a shift in the $y$ direction. Note: For lines there are always an analog transformations on the $x$ direction.]

```{r}
#| echo: true
#| output-location: column
a <- 0.5
b <- 1
func <- function(x) a*x + b
ggplot() + geom_function(fun = func, size = 2) +
 # Set axis limits and make axis equal
	xlim(c(-0.5,2)) + ylim(c(0,2)) + coord_fixed() + 
	geom_line( # intercept line:
	 data=tibble(x=c(0,0),y=c(0,1)), 
	 mapping = aes(x,y), 
	 color = "blue", size = 2) +
	geom_line( # slope:
	 data=tibble(x=c(1.5,1.5),y=c(1.25,1.75)), 
	 mapping = aes(x,y), 
	 color = "red", size = 2) +
	geom_line( # x-interval of length one:
	 data=tibble(x=c(0.5,1.5),y=c(1.25,1.25)), 
	 mapping = aes(x,y), color = "gray") +
	theme_classic(base_size = 24)
```

## Penguins: Linear model {.smaller}

**Flipper length** as a function of **body mass**.

```{r}
#| echo: true
#| output-location: column
#| fig-height: 9
penguins |>
 ggplot(aes(x = body_mass_g, 
            y = flipper_length_mm)) +
 geom_point() +
 geom_smooth(method = "lm", 
             se = FALSE) + 
 theme_classic(base_size = 24)
```

## Penguins: A smooth line {.smaller}

**Flipper length** as a function of **body mass** with `loess`^[loess = locally estimated scatterplot smoothing] smoothing. 

```{r}
#| echo: true
#| output-location: column
#| fig-height: 7
penguins |>
 ggplot(aes(x = body_mass_g, 
            y = flipper_length_mm)) +
 geom_point() +
 geom_smooth(method = "loess") + 
 theme_classic(base_size = 24)
```

This is a less theory-driven and more data-driven model. Why?    
[We don't have a simple mathematical form of the function.]{.fragment}

## Terminology variable-based models {.smaller background-color="khaki"}

- **Response variable:**^[Also **dependent variable** in statistics or empirical social sciences.] Variable whose behavior or variation you are trying to understand, on the y-axis
- **Explanatory variable(s):**^[Also **independent variable(s)** in statistics or empirical social sciences.] Other variable(s) that you want to use to explain the variation in the response, on the x-axis
- **Predicted value:** Output of the model function. 
  - The model function gives the **(expected) average value** of the response variable conditioning on the explanatory variables
  - **Residual(s):** A measure of how far away a case is from its predicted value (based on the particular model)   
    Residual = Observed value - Predicted value  
    The residual tells how far above/below the expected value each case is

## More explanatory variables {.smaller}

How does the relation between flipper length and body mass change with different species?

```{r}
#| echo: true
#| output-location: column
#| fig-height: 7
penguins |>
 ggplot(aes(x = body_mass_g, 
            y = flipper_length_mm, 
            color = species)) +
 geom_point() +
 geom_smooth(method = "lm",
             se = FALSE) + 
 theme_classic(base_size = 24)
```

## ggplot-hint: How to color penguins but keep one model? {.smaller}

Put the mapping of the color aesthetic into the `geom_point` command. 

```{r}
#| echo: true
#| output-location: column
#| fig-height: 6
penguins |>
 ggplot(aes(x = body_mass_g, 
            y = flipper_length_mm)) +
 geom_point(aes(color = species)) +
 geom_smooth(method = "lm",	se = FALSE) + 
 theme_classic(base_size = 24)
```

## Beware of Simpson's paradox {.smaller background-color="aquamarine"} 

Slopes for all groups can be in the opposite direction of the main effect's slope!

![](https://upload.wikimedia.org/wikipedia/commons/f/fb/Simpsons_paradox_-_animation.gif)
::: aside
Source: <https://upload.wikimedia.org/wikipedia/commons/f/fb/Simpsons_paradox_-_animation.gif>
:::


## The paradox is real! {.smaller}

How does the relation between bill length and body mass change with different species?

```{r}
#| echo: true
#| output-location: column
#| fig-height: 7
penguins |>
 ggplot(aes(x = bill_length_mm, 
            y = bill_depth_mm)) +
 geom_point(aes(color = species)) +
 geom_smooth(mapping = aes(color = species),
             method = "lm",
             se = FALSE) + 
 geom_smooth(method = "lm",
													se = FALSE) + 
 theme_classic(base_size = 24)
```


## Models - upsides and downsides {.smaller background-color="khaki"}

- Models can reveal patterns that are not evident in a graph of the data. This is an advantage of modeling over simple visual inspection of data.
  - How would you visualize dependencies of more than two variables?
- The risk is that a model is imposing structure that is not really there in the real world data. 
  - People imagined animal shapes in the stars. This is maybe a good model to detect and memorize shapes, but it has nothing to do with these animals.
  - Every model is a simplification of the real world, but there are good and bad models (for particular purposes). 
  - A skeptical (but constructive) approach to a model is always advisable. 
  
  
## Variation around a model {.smaller background-color="khaki"}

... is as interesting and important as the model!

*Statistics is the explanation of uncertainty of variation in the context of what remains unexplained.*

- The scattered data of flipper length and body mass suggests that there maybe other factors that account for some parts of the variability. 
- Or is it randomness?
- Adding more explanatory variables can help (but need not)

## *All models are wrong ...* {.smaller background-color="khaki"}

*... but some are useful.* (George Box)


Extending the range of the model: 

```{r}
#| echo: true
#| output-location: column
#| fig-height: 5.5
penguins |>
 ggplot(aes(x = body_mass_g, 
            y = flipper_length_mm)) +
 geom_point() +
 geom_smooth(method = "lm", 
             se = FALSE, 
 												fullrange = TRUE) +
	xlim(c(0,7000)) + ylim(c(0,230)) +
 theme_classic(base_size = 24)
```

- The model predicts that penguins with zero weight still have flippers of about 140 mm on average.
- Is the model useless? [Yes, around zero body mass. No, it works OK in the range of the body mass data.]{.fragment}

## Two model purposes {.smaller background-color="khaki"}

Linear models can be used for:

- **Explanation:** Understand the relationship of variables in a quantitative way.   
*For the linear model, interpret slope and intercept.*
- **Prediction:** Plug in new values for the explanatory variable(s) and receive the expected response value.   
*For the linear model, predict the flipper length of new penguins by their body mass.*

# Fitting Models (Part 1)

Today: The linear model. 

## In R: `tidymodels`

![](https://datasciencebox.org/course-materials/_slides/u4-d02-fitting-interpreting-models/img/tidymodels.png)

:::{.aside}
From <https://datasciencebox.org>
:::

## Our goal

Predict flipper length from body mass

average `flipper_length_mm` $= \beta_0 + \beta_1\cdot$ `body_mass_g`


## Step 1: Specify model

```{r}
#| echo: true
library(tidymodels)
linear_reg()
```

## Step 2: Set the model fitting *engine*

```{r}
#| echo: true
linear_reg() |> 
	set_engine("lm")
```

## Step 3: Fit model and estimate parameters {.smaller}

Only now, the data and the variable selection comes in. 

Use of **formula syntax**

```{r}
#| echo: true
linear_reg() |> 
	set_engine("lm") |> 
	fit(flipper_length_mm ~ body_mass_g, data = penguins)
```

[`parsnip`](http://parsnip.tidymodels.org) is package in `tidymodels` which is to provide a tidy, unified interface to models that can be used to try a range of models. 

:::{.aside}
Note: The fit command does not follow the tidyverse principle the data comes first. Instead, the formula comes first. This is to relate to existing traditions of a much older established way of modeling in base R. 
:::

## What does the output say? {.smaller}

```{r}
#| echo: true
linear_reg() |> 
	set_engine("lm") |> 
	fit(flipper_length_mm ~ body_mass_g, data = penguins)
```

. . .

average `flipper_length_mm` $= 136.72956 + 0.01528\cdot$ `body_mass_g`

. . .

**Interpretation:**   
The penguins have a flipper length of 138 mm plus 0.01528 mm for each gram of body mass (that is 15.28 mm per kg).
Penguins with zero mass have a flipper length of 138 mm. However, this is not in the range where the model was fitted.

## Show output in *tidy* form

```{r}
#| echo: true
linear_reg() |> 
	set_engine("lm") |> 
	fit(flipper_length_mm ~ body_mass_g, data = penguins) |> 
	tidy()
```

## Parameter estimation {.smaller background-color="aquamarine"}

Notation from statistics: $\beta$'s for the population parameters and $\hat\beta$'s for the parameters estimated from the sample statistics. 

$$\hat y = \beta_0 + \beta_1 x$$

Is what we cannot have. ($\hat y$ stands for *predicted value of $y$*. )

. . .

We estimate $\hat\beta_0$ and $\hat\beta_1$ in the model fitting process.

$$\hat y = \hat\beta_0 + \hat\beta_1 x$$

:::{style="background-color:khaki;padding:10px;"}
A typical follow-up data analysis question is what the fitted values $\hat\beta_0$ and $\hat\beta_1$ tell us about the population-wide values $\beta_0$ and $\beta_1$? 

This is the typical [**inferential question**]{style='color:blue}.
::: 



# Fitting Models (Part 2)

Today: The linear model. 

```{r}
#| include: true
library(tidyverse)
library(palmerpenguins)
library(tidymodels)
```

## Recap: Linear model {.smaller}

**Flipper length** as a function of **body mass**.

```{r}
#| echo: true
#| output-location: column
#| fig-height: 7
penguins |>
 ggplot(aes(x = body_mass_g, 
            y = flipper_length_mm)) +
 geom_point() +
 geom_smooth(method = "lm", 
             se = FALSE) + 
 theme_classic(base_size = 24)
```

. . . 

The fitting question: [How can we get this line?]{style='color:red;'}

## Recap: A line {.smaller background-color="aquamarine"}

A *line* is a shift-scale transformation of the identity function usually written in the form 

$$f(x) = a\cdot x + b$$

where [$a$ is the *slope*]{style="color:red;"}, [$b$ is the *intercept*]{style="color:blue;"}.

```{r}
#| fig-height: 4
a <- 0.5
b <- 1
func <- function(x) a*x + b
ggplot() + geom_function(fun = func, size = 2) +
 # Set axis limits and make axis equal
	xlim(c(-0.5,2)) + ylim(c(0,2)) + coord_fixed() + 
	geom_line( # intercept line:
	 data=tibble(x=c(0,0),y=c(0,1)), 
	 mapping = aes(x,y), 
	 color = "blue", size = 2) +
	geom_line( # slope:
	 data=tibble(x=c(1.5,1.5),y=c(1.25,1.75)), 
	 mapping = aes(x,y), 
	 color = "red", size = 2) +
	geom_line( # x-interval of length one:
	 data=tibble(x=c(0.5,1.5),y=c(1.25,1.25)), 
	 mapping = aes(x,y), color = "gray") +
	theme_classic(base_size = 24)
```

## Recap: Terminology {.smaller background-color="khaki"}

- **Response variable:**^[Also **dependent variable** in statistics or empirical social sciences.] Variable whose behavior or variation you are trying to understand, on the y-axis
- **Explanatory variable(s):**^[Also **independent variable(s)** in statistics or empirical social sciences.] Other variable(s) that you want to use to explain the variation in the response, on the x-axis
- **Predicted value:** Output of the model function. 
  - The model function gives the **(expected) average value** of the response variable conditioning on the explanatory variables
  - **Residual(s):** A measure of how far away a case is from its predicted value (based on the particular model)   
    Residual = Observed value - Predicted value  
    The residual tells how far above/below the expected value each case is



## Our goal

Predict flipper length from body mass

average `flipper_length_mm` $= \beta_0 + \beta_1\cdot$ `body_mass_g`

. . . 

$\beta_0$ is the *intercet* of the line

$\beta_1$ is the *slope* of the line 

. . . 

Later we will have $\beta_2, \dots, \beta_m$ as coefficients for more variables. 




## Fitting the model in R {.smaller}

```{r}
#| echo: true
linear_reg() |> 
	set_engine("lm") |> 
	fit(flipper_length_mm ~ body_mass_g, data = penguins)
```

average `flipper_length_mm` $= 136.72956 + 0.01528\cdot$ `body_mass_g`

**Interpretation:**   
The penguins have a flipper length of 136.7 mm plus 0.01528 mm for each gram of body mass (that is 15.28 mm per kg).
Penguins with zero mass have a flipper length of 136.7 mm. However, this is not in the range where the model was fitted.

## `parsnip` model objects {.smaller}

```{r}
#| echo: true
pengmod <- linear_reg() |> 
	set_engine("lm") |> 
	fit(flipper_length_mm ~ body_mass_g, data = penguins)
class(pengmod) # attributes
typeof(pengmod) 
names(pengmod)
```

. . . 

Most interesting for us for now: `$fit`

```{r}
#| echo: true
pengmod$fit
```

. . . 

Notice: `parsnip model object` is now missing in the output.

## `$fit` is the object created by `lm` (base R) {.smaller}

```{r}
#| echo: true
names(pengmod$fit)
pengmod$fit$call
pengmod$fit$coefficients
pengmod$fit$fitted.values |> head()
pengmod$fit$residuals |> head()
```

## Fitting method: Least squares regression {.smaller background-color="aquamarine"}

- The regression line shall minimize the sum of the squared residuals     
  (or, identically, their mean). 
- Mathematically: The residual for case $i$ is $e_i = \hat y_i - y_i$. 
- Now we want to minimize $\sum_{i=1}^n e_i^2$   
(or equivalently $\frac{1}{n}\sum_{i=1}^n e_i^2$ the *the mean of squared errors*, which we will look at later). 

## Visualization of residuals {.smaller background-color="aquamarine"}

The residuals are the gray lines between predictid values on the regression line and the actual values. 

```{r}
pengmod <- 
  linear_reg() |> 
  set_engine("lm") |> 
  fit(flipper_length_mm ~ body_mass_g, data = penguins)
penguins |> bind_cols(predict(pengmod,penguins)) |> 
  ggplot(aes(body_mass_g, flipper_length_mm)) +
  geom_segment(aes(x = body_mass_g, y = flipper_length_mm, xend = body_mass_g, yend = .pred), color = "gray") +
  geom_point(alpha = 0.3) +
  geom_smooth(method="lm") + 
  geom_point(aes(y=.pred), color = "red", alpha =  0.3)
```

## Check: Fitted values and Residuals {.smaller background-color="aquamarine"}

Recall: **Residual = Observed value - Predicted value**

The *Predicted values* are also called *Fitted values*. Hence: 

Residuals + Fitted values = Observed values

```{r}
#| echo: true
(pengmod$fit$residuals + pengmod$fit$fitted.values) |> 
head()
```

```{r}
#| echo: true
penguins$flipper_length_mm |> head()
```



## Proporties of least squares regression  {.smaller background-color="aquamarine"}

The regression lines goes through the point (`mean(x)`, `mean(y)`). 

```{r}
#| echo: true
mean(penguins$body_mass_g, na.rm = TRUE)
mean(penguins$flipper_length_mm, na.rm = TRUE)
```

```{r}
penguins |> bind_cols(predict(pengmod,penguins)) |> 
  ggplot(aes(body_mass_g, flipper_length_mm)) +
  geom_segment(aes(x = body_mass_g, y = flipper_length_mm, xend = body_mass_g, yend = .pred), color = "gray") +
  geom_point(alpha = 0.3) +
  geom_smooth(method="lm") + 
  geom_point(aes(y=.pred), color = "red", alpha =  0.3) + 
  geom_point(data = tibble(x = mean(penguins$body_mass_g, na.rm = T), y = mean(penguins$flipper_length_mm, na.rm = T)), 
  											mapping = aes(x,y), color = "green", size = 5)
```


## Proporties of least squares regression  {.smaller background-color="aquamarine"}

Residuals sum up to zero 

```{r}
#| echo: true
pengmod <- linear_reg() |>  set_engine("lm") |> fit(flipper_length_mm ~ body_mass_g, data = penguins)
pengmod$fit$residuals |> sum()
```

. . .

There is no correlation between residuals and the explanatory variable 

```{r}
#| echo: true
cor(pengmod$fit$residuals, na.omit(penguins$body_mass_g))
```

. . .

The correlation of $x$ and $y$ is the slope $\beta_1$ corrected by their standard deviations. 

```{r}
#| echo: true
correlation <- cor(penguins$flipper_length_mm, penguins$body_mass_g, use = "pairwise.complete.obs")
sd_flipper <- sd(penguins$flipper_length_mm, na.rm = T)
sd_mass <- sd(penguins$body_mass_g, na.rm = T)
c(correlation, sd_flipper, sd_mass)

correlation * sd_flipper / sd_mass

pengmod$fit$coefficients
```


## Correlation and linear regression {.smaller background-color="aquamarine"}

When the two variables in the linear regression are standardized (standard scores)

- the intercept is zero 
- the coefficient coincides with the correlation



# Linear Models and Dummy Variables


## Explanatory variables are categorical {.smaller}

Let's just try what happens with `species` as explanatory variable. 
Remember, we have three species: Adelie, Chinstrap, Gentoo.

```{r}
#| echo: true
linear_reg() |> 
	set_engine("lm") |> 
	fit(flipper_length_mm ~ species, data = penguins) |> 
	tidy()
```

What happened? 

. . . 

Two of the three species categories appear as variables now.

- Categorical variables are automatically encoded to **dummy variables**
- Each coefficient describes the expected difference between flipper length of that particular species compared to the baseline level
- What is the baseline level? [[The first category!]{style='color:red;'} (Here alphabetically `"Adelie"`)]{.fragment}


## How do dummy variables look? {.smaller}

species    | speciesChinstrap | speciesGentoo 
-----------|------------------|--------------
Adelie     |      0           | 0
Chinstrap  |      1           | 0
Gentoo     |      0           | 1

Then the fitting of the linear model is as before using the zero-one variables. 

## Interpretation {.smaller background-color="khaki"}

```{r}
#| echo: true
linear_reg() |> 
	set_engine("lm") |> 
	fit(flipper_length_mm ~ species, data = penguins) |> 
	tidy()
```

- Flipper length of the baseline species is the intercept.    
    - Average flipper length of Adelie is 190 mm
- Flipper length of the two other species add their coefficient
    - Average flipper length of Chinstrap is 190 + 5.87 mm
    - Average flipper length of Gentoo is 190 + 27.2 mm


## Compare to a visualization {.smaller}

```{r}
linear_reg() |> 
	set_engine("lm") |> 
	fit(flipper_length_mm ~ species, data = penguins) |> 
	tidy()
```

```{r}
#| fig-width: 3
penguins |> 
  ggplot(aes(species, flipper_length_mm)) + geom_boxplot() +
 	stat_summary(fun.y=mean, geom="point", size=5, color="red")
```

The red dots are the average values for species. 

# Linear models and R-squared

## Linear model {.smaller} 

```{r}
#| echo: true
linear_reg() |> set_engine("lm") |> 
	fit(flipper_length_mm ~ body_mass_g, data = penguins) |> 
 tidy()
```
average `flipper_length_mm` $= 137 + 0.0153\cdot$ `body_mass_g`

. . .

```{r}
#| echo: true
linear_reg() |> set_engine("lm") |> 
	fit(bill_depth_mm ~ bill_length_mm, data = penguins) |> 
 tidy()
```

average `bill_depth_mm` $= 20.9 -0.085\cdot$ `bill_length_mm`

. . .

**Technical:** The idea of the `tidy()` function is to turn an object into a tidy tibble. Here, it extracts the coefficients of the linear model (and more statistical information).




## R-squared of a fitted model {.smaller background-color="aquamarine"}

$R^2$ is the percentage of variability in the response explained by the regression model. 

R-squared is also called **coefficient of determination**. 

**Definition**: 

$R^2 = 1 - \frac{SS_\text{res}}{SS_\text{tot}}$

where $SS_\text{res} = \sum_i(y_i - f_i)^2 = \sum_i e_i^2$ is the *sum of the squared residuals*, and   
$SS_\text{tot} = \sum_i(y_i - \bar y)^2$ the *total sum of squares* which is proportional to the variance of $y$. ($\bar y$ is the mean of $y$.)

![](https://upload.wikimedia.org/wikipedia/commons/8/86/Coefficient_of_Determination.svg)



## Linear model R-squared {.smaller}

```{r}
#| echo: true
linear_reg() |> set_engine("lm") |> 
 fit(flipper_length_mm ~ body_mass_g, data = penguins) |>
 glance()  # glance shows summary statistics of model fit
```

**Interpretation R-squared?** [75.9% of the variance of flipper length can be explained by a linear relation with body mass. ]{.fragment}

. . .

```{r}
#| echo: true
linear_reg() |> set_engine("lm") |> 
	fit(bill_depth_mm ~ bill_length_mm, data = penguins) |> 
 glance()
```

5.52% of the variance of bill depth can be explained by a linear relation with bill length. 

. . . 

**Technical:** The idea of the `glance()` function is to construct a single row summary "glance" of a model, fit, or other object. 

## R-squared and correlation {.smaller}

For a linear model with one predictor, the square of the correlation coefficient is the same as the R-squared of the model. 

```{r}
#| echo: true
linear_reg() |> set_engine("lm") |> 
	fit(flipper_length_mm ~ body_mass_g, data = penguins) |> 
 glance()
```

```{r}
#| echo: true
cor(penguins$flipper_length_mm, penguins$body_mass_g, use = "pairwise.complete.obs")
```

```{r}
#| echo: true
cor(penguins$flipper_length_mm, penguins$body_mass_g, use = "pairwise.complete.obs")^2
```

. . . 

Hence, the name $R^2$!

# Linear models with more predictors

## More predictors: Coefficients {.smaller}

```{r}
#| echo: true
linear_reg() |> set_engine("lm") |> 
 fit(flipper_length_mm ~ body_mass_g, data = penguins) |> 
 tidy()
```

```{r}
#| echo: true
linear_reg() |> set_engine("lm") |> 
 fit(flipper_length_mm ~ body_mass_g + bill_length_mm, data = penguins) |> 
 tidy()
```

```{r}
#| echo: true
linear_reg() |> set_engine("lm") |> 
 fit(flipper_length_mm ~ body_mass_g + bill_length_mm + bill_depth_mm, data = penguins) |> 
 tidy()
```

## More predictors: `glance` R-squared {.smaller}

```{r}
#| echo: true
linear_reg() |> set_engine("lm") |> 
 fit(flipper_length_mm ~ body_mass_g, data = penguins) |> 
 glance()
```

```{r}
#| echo: true
linear_reg() |> set_engine("lm") |> 
 fit(flipper_length_mm ~ body_mass_g + bill_length_mm, data = penguins) |> 
 glance()
```

```{r}
#| echo: true
linear_reg() |> set_engine("lm") |> 
 fit(flipper_length_mm ~ body_mass_g + bill_length_mm + bill_depth_mm, data = penguins) |> 
 glance()
```

##  More predictors: Equations {.smaller}

average `flipper_length_mm` $= 137 + 0.0153\cdot$ `body_mass_g`    
75.9% explained variance

average `flipper_length_mm` $= 122 + 0.0131\cdot$ `body_mass_g` $+ 0.549\cdot$ `bill_length_mm`    
78.8% explained variance

average `flipper_length_mm` $= 158 + 0.0109\cdot$ `body_mass_g` $+ 0.592\cdot$ `bill_length_mm` $- 1.68\cdot$ `bill_length_mm`   
83.1% explained variance


## Adding a categorical variable as main effect {.smaller}

```{r}
#| echo: true
linear_reg() |> set_engine("lm") |> 
 fit(flipper_length_mm ~ body_mass_g + species, data = penguins) |> 
 tidy()
```

A **main effect** by categorical dummy variables allows for different intercepts per species. (However, the slopes are the same.)


```{r}
#| echo: true
linear_reg() |> set_engine("lm") |> 
 fit(flipper_length_mm ~ body_mass_g + species, data = penguins) |> 
 glance()
```

Adding species increases R-squared better than adding bill length and bill depth together!


## Adding as interaction effect {.smaller}

```{r}
#| echo: true
linear_reg() |> set_engine("lm") |> 
 fit(flipper_length_mm ~ body_mass_g * species, data = penguins) |> 
 tidy()
```

- Note the `*` for interaction effect!
- Also main effects for both variables are in as coefficients.
- Adelie is the baseline species (because it is first in the alphabet).
- An **interaction effect** allows for different slopes for each species!


## Improvement through the interaction effects is small here {.smaller}

```{r}
#| echo: true
linear_reg() |> set_engine("lm") |> 
 fit(flipper_length_mm ~ body_mass_g + species, data = penguins) |> 
 glance()
```

```{r}
#| echo: true
linear_reg() |> set_engine("lm") |> 
 fit(flipper_length_mm ~ body_mass_g * species, data = penguins) |> 
 glance()
```


## Regression lines by species {.smaller}

```{r}
penguins |> 
  ggplot(aes(x = body_mass_g, y = flipper_length_mm, color = species)) +
  geom_point() + geom_smooth(method = "lm") +
  theme_minimal()
```

Compare the slopes to the regression output on the slides before!

## Different equations for each species! {.smaller}

```{r}
#| echo: true
linear_reg() |> set_engine("lm") |> 
 fit(flipper_length_mm ~ body_mass_g * species, data = penguins) |> 
 tidy()
```

Adelie:   
average `flipper_length_mm` $= 165 + 0.00668\cdot$ `body_mass_g`    

Chinstrap:   
average `flipper_length_mm` $= 165 - 13.6 + (0.00668 + 0.00523)\cdot$ `body_mass_g`    

Gentoo:   
average `flipper_length_mm` $= 165 + 6.06 + (0.00668 + 0.00236)\cdot$ `body_mass_g`    

## Interaction effects more categoricals {.smaller}

Adding products of variables in the linear model 
$y_i = \beta_0 + \beta_1x_1  + \beta_2x_2 + \beta_{3}x_1x_2 + \dots$. 

For $x_1$ and $x_2$ being dummy variables for *being female* and *having kids* this is for example

```{r}
tibble(gndr_f = c(0,1,1,0), has_kids = c(1,0,1,0)) |> 
 mutate(gndr_f_x_has_kids = gndr_f * has_kids) |> 
 knitr::kable()
```

- What is the baseline? [*Being male without kids.*]{.fragment}

Thought experiment: 

- When we estimate a model explaining life satisfaction with these. How would we see if being a mother increases life satisfaction more than being a father? [*positiv coefficient for `gndr_f_x_has_kids`*]{.fragment}


# Nonlinear Models

## When a linear model is bad

Example: Total corona cases in Germany in the first wave 2020. 

```{r}
whofull <- read_csv("data/WHO-COVID-19-global-data.csv", show_col_types = FALSE) |> 
	filter(Country == "Germany") 
who <- whofull |> 
	filter(Date_reported < "2020-03-20", Date_reported > "2020-02-25") 
who |> 
	ggplot(aes(Date_reported, Cumulative_cases)) + 
	geom_line() + geom_point() + geom_smooth(method = "lm")
```

## $\log$ transformation {.smaller}

Instead of `Cumulative_cases` we look at $\log($`Cumulative_cases`$)$
```{r}
	who |> 
	ggplot(aes(Date_reported, log(Cumulative_cases))) + geom_line() + geom_point() +
	geom_smooth(method = "lm") + theme_minimal(base_size = 20)
```

Almost perfect fit of the linear model: $\log(y)=\log(\beta_0) + \beta_1\cdot x$   
($y=$ `Cumulative cases`, $x=$ Days)

. . . 

Exponentiation gives the model: $y=\beta_0 e^{\beta_1\cdot x}$  (Check $e^{\log(\beta_0) + \beta_1\cdot x} = e^{\log(\beta_0)} e^{\beta_1\cdot x} = \beta_0 e^{\beta_1\cdot x}$)

## Exponential growth! {.smaller}

[$y=\beta_0 e^{\beta_1\cdot x}$]{style='color:red;'}  
For comparison: Logistic function [$y = \frac{N \beta_0 e^{\beta_1\cdot x}}{N + \beta_0 e^{\beta_1\cdot x}}$]{style='color:blue;'}   for $N=200000$

```{r}
wholm <- 	who |> 
 lm(log(Cumulative_cases) ~ Date_reported, data = _) 
who |> 
	ggplot(aes(Date_reported, Cumulative_cases)) + geom_line() + geom_point() +
 geom_function(fun = function(x) exp(coef(wholm)[1] + coef(wholm)[2]*as.numeric(x)), color = "red") + 
 geom_function(
  fun = function(x) 200000*exp(coef(wholm)[1] + coef(wholm)[2]*as.numeric(x))/(200000 + exp(coef(wholm)[1] + coef(wholm)[2]*as.numeric(x))), color = "blue") +
 theme_minimal(base_size = 20)
```

Logistic growth (as in the SI model) mimicks exponential growth initially. 
 

## $\log$ transformation {.smaller}

```{r}
	who |> 
	ggplot(aes(Date_reported, log10(Cumulative_cases))) + geom_line() + geom_point() +
	geom_smooth(method = "lm") 
```

What is the difference to the penguin model?

. . . 

- $x$ has an ordered structure and no duplicates

The fit looks so good. Why?

. . . 

Because there is a *mechanistic explanation* behind: The SI model. 

## However, it works only in a certain range {.smaller}

```{r}
who |> 
	ggplot(aes(Date_reported, Cumulative_cases)) + geom_point() +
 geom_function(fun = function(x) exp(coef(wholm)[1] + coef(wholm)[2]*as.numeric(x)), color = "red") + 
 geom_function(
  fun = function(x) 200000*exp(coef(wholm)[1] + coef(wholm)[2]*as.numeric(x))/(200000 + exp(coef(wholm)[1] + coef(wholm)[2]*as.numeric(x))), color = "blue") +
 theme_minimal(base_size = 20) +
 geom_line(data = whofull |> filter(Date_reported < "2020-06-30")) + ylim(c(0,300000))
```

## However, it works only in a certain range (log scale on y) {.smaller}

```{r}
who |> 
	ggplot(aes(Date_reported, Cumulative_cases)) + geom_point() +
 geom_function(fun = function(x) exp(coef(wholm)[1] + coef(wholm)[2]*as.numeric(x)), color = "red") + 
 geom_function(
  fun = function(x) 200000*exp(coef(wholm)[1] + coef(wholm)[2]*as.numeric(x))/(200000 + exp(coef(wholm)[1] + coef(wholm)[2]*as.numeric(x))), color = "blue") +
 theme_minimal(base_size = 20) +
 geom_line(data = whofull |> filter(Date_reported < "2020-06-30")) + ylim(c(0,300000)) +
 scale_y_log10()
```


